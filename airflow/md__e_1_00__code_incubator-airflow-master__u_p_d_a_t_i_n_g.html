<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>airflow_analysis: Updating Airflow</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">airflow_analysis
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

</div><!-- top -->
<div class="header">
  <div class="headertitle">
<div class="title">Updating Airflow </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This file documents any backwards-incompatible changes in Airflow and assists people when migrating to a new version.</p>
<h2>Master</h2>
<h3>New Features</h3>
<h4>Dask Executor</h4>
<p>A new DaskExecutor allows Airflow tasks to be run in Dask Distributed clusters.</p>
<h3>Deprecated Features</h3>
<p>These features are marked for deprecation. They may still work (and raise a <code>DeprecationWarning</code>), but are no longer supported and will be removed entirely in Airflow 2.0</p>
<ul>
<li><p class="startli"><code>post_execute()</code> hooks now take two arguments, <code>context</code> and <code>result</code> (AIRFLOW-886)</p>
<p class="startli">Previously, post_execute() only took one argument, <code>context</code>.</p>
</li>
</ul>
<h2>Airflow 1.8</h2>
<h3>Database</h3>
<p>The database schema needs to be upgraded. Make sure to shutdown Airflow and make a backup of your database. To upgrade the schema issue <code>airflow upgradedb</code>.</p>
<h3>Upgrade systemd unit files</h3>
<p>Systemd unit files have been updated. If you use systemd please make sure to update these.</p>
<blockquote class="doxtable">
<p>Please note that the webserver does not detach properly, this will be fixed in a future version. </p>
</blockquote>
<h3>Tasks not starting although dependencies are met due to stricter pool checking</h3>
<p>Airflow 1.7.1 has issues with being able to over subscribe to a pool, ie. more slots could be used than were available. This is fixed in Airflow 1.8.0, but due to past issue jobs may fail to start although their dependencies are met after an upgrade. To workaround either temporarily increase the amount of slots above the the amount of queued tasks or use a new pool.</p>
<h3>Less forgiving scheduler on dynamic start_date</h3>
<p>Using a dynamic start_date (e.g. <code>start_date = <a class="el" href="namespacetest__dag.html#ae3630195185093e7e615157105d3b9f1">datetime.now()</a></code>) is not considered a best practice. The 1.8.0 scheduler is less forgiving in this area. If you encounter DAGs not being scheduled you can try using a fixed start_date and renaming your dag. The last step is required to make sure you start with a clean slate, otherwise the old schedule can interfere.</p>
<h3>New and updated scheduler options</h3>
<p>Please read through these options, defaults have changed since 1.7.1.</p>
<h4>child_process_log_directory</h4>
<p>In order the increase the robustness of the scheduler, DAGS our now processed in their own process. Therefore each DAG has its own log file for the scheduler. These are placed in <code>child_process_log_directory</code> which defaults to <code>&lt;AIRFLOW_HOME&gt;/scheduler/latest</code>. You will need to make sure these log files are removed.</p>
<blockquote class="doxtable">
<p>DAG logs or processor logs ignore and command line settings for log file locations. </p>
</blockquote>
<h4>run_duration</h4>
<p>Previously the command line option <code>num_runs</code> was used to let the scheduler terminate after a certain amount of loops. This is now time bound and defaults to <code>-1</code>, which means run continuously. See also num_runs.</p>
<h4>num_runs</h4>
<p>Previously <code>num_runs</code> was used to let the scheduler terminate after a certain amount of loops. Now num_runs specifies the number of times to try to schedule each DAG file within <code>run_duration</code> time. Defaults to <code>-1</code>, which means try indefinitely. This is only available on the command line.</p>
<h4>min_file_process_interval</h4>
<p>After how much time should an updated DAG be picked up from the filesystem.</p>
<h4>dag_dir_list_interval</h4>
<p>How often the scheduler should relist the contents of the DAG directory. If you experience that while developing your dags are not being picked up, have a look at this number and decrease it when necessary.</p>
<h4>catchup_by_default</h4>
<p>By default the scheduler will fill any missing interval DAG Runs between the last execution date and the current date. This setting changes that behavior to only execute the latest interval. This can also be specified per DAG as <code>catchup = False / True</code>. Command line backfills will still work.</p>
<h3>Faulty Dags do not show an error in the Web UI</h3>
<p>Due to changes in the way Airflow processes DAGs the Web UI does not show an error when processing a faulty DAG. To find processing errors go the <code>child_process_log_directory</code> which defaults to <code>&lt;AIRFLOW_HOME&gt;/scheduler/latest</code>.</p>
<h3>New DAGs are paused by default</h3>
<p>Previously, new DAGs would be scheduled immediately. To retain the old behavior, add this to airflow.cfg:</p>
<div class="fragment"><div class="line">[core]</div><div class="line">dags_are_paused_at_creation = False</div></div><!-- fragment --><h3>Airflow Context variable are passed to Hive config if conf is specified</h3>
<p>If you specify a hive conf to the run_cli command of the HiveHook, Airflow add some convenience variables to the config. In case your run a sceure Hadoop setup it might be required to whitelist these variables by adding the following to your configuration:</p>
<div class="fragment"><div class="line">&lt;property&gt; </div><div class="line">     &lt;name&gt;hive.security.authorization.sqlstd.confwhitelist.append&lt;/name&gt;</div><div class="line">     &lt;value&gt;airflow\.ctx\..*&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></div><!-- fragment --> <h3>Google Cloud Operator and Hook alignment</h3>
<p>All Google Cloud Operators and Hooks are aligned and use the same client library. Now you have a single connection type for all kinds of Google Cloud Operators.</p>
<p>If you experience problems connecting with your operator make sure you set the connection type "Google Cloud Platform".</p>
<p>Also the old P12 key file type is not supported anymore and only the new JSON key files are supported as a service account.</p>
<h3>Deprecated Features</h3>
<p>These features are marked for deprecation. They may still work (and raise a <code>DeprecationWarning</code>), but are no longer supported and will be removed entirely in Airflow 2.0</p>
<ul>
<li><p class="startli">Hooks and operators must be imported from their respective submodules</p>
<p class="startli"><code>airflow.operators.PigOperator</code> is no longer supported; <code>from <a class="el" href="namespaceairflow_1_1operators_1_1pig__operator.html">airflow.operators.pig_operator</a> import PigOperator</code> is. (AIRFLOW-31, AIRFLOW-200)</p>
</li>
<li><p class="startli">Operators no longer accept arbitrary arguments</p>
<p class="startli">Previously, <code>Operator.__init__()</code> accepted any arguments (either positional <code>*args</code> or keyword <code>**kwargs</code>) without complaint. Now, invalid arguments will be rejected. (<a href="https://github.com/apache/incubator-airflow/pull/1285">https://github.com/apache/incubator-airflow/pull/1285</a>)</p>
</li>
</ul>
<h3>Known Issues</h3>
<p>There is a report that the default of "-1" for num_runs creates an issue where errors are reported while parsing tasks. It was not confirmed, but a workaround was found by changing the default back to <code>None</code>.</p>
<p>To do this edit <code><a class="el" href="cli_8py.html">cli.py</a></code>, find the following:</p>
<div class="fragment"><div class="line">&#39;num_runs&#39;: Arg(</div><div class="line">    (&quot;-n&quot;, &quot;--num_runs&quot;),</div><div class="line">    default=-1, type=int,</div><div class="line">    help=&quot;Set the number of runs to execute before exiting&quot;),</div></div><!-- fragment --><p>and change <code>default=-1</code> to <code>default=None</code>. Please report on the mailing list if you have this issue.</p>
<h2>Airflow 1.7.1.2</h2>
<h3>Changes to Configuration</h3>
<h4>Email configuration change</h4>
<p>To continue using the default smtp email backend, change the email_backend line in your config file from:</p>
<div class="fragment"><div class="line">[email]</div><div class="line">email_backend = airflow.utils.send_email_smtp</div></div><!-- fragment --><p> to: </p><div class="fragment"><div class="line">[email]</div><div class="line">email_backend = airflow.utils.email.send_email_smtp</div></div><!-- fragment --><h4>S3 configuration change</h4>
<p>To continue using S3 logging, update your config file so:</p>
<div class="fragment"><div class="line">s3_log_folder = s3://my-airflow-log-bucket/logs</div></div><!-- fragment --><p> becomes: </p><div class="fragment"><div class="line">remote_base_log_folder = s3://my-airflow-log-bucket/logs</div><div class="line">remote_log_conn_id = &lt;your desired s3 connection&gt;</div></div><!-- fragment --> </div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
